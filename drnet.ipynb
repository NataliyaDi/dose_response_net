{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Here is an implementation of a neural network for solving the problem of finding the optimal treatment size per person.\n",
    "\n",
    "\n",
    "**problem:**\n",
    "\n",
    "It is often too costly for companies to prescribe treatment randomly. In this regard, many classical approaches don't work correctly (_uplift trees, S/T/X learners etc_).\n",
    "Person get the treatment according features.\n",
    "\n",
    "**solution:**\n",
    "This neural network is trying to learn how to cope with this non-randomness.\n",
    "* First, the network is trained on all objects,\n",
    "* then we devide batch into treatment and control groups,\n",
    "* then we devide batch according to the magnitude of the treatment (here we'd like to find the effect of feature values on treatment assignment),\n",
    "* then predict target and count loss."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (16, 8)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import clear_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set defaults\n",
    "all_features = []\n",
    "cat_features = []\n",
    "cont_features = [x for x in all_features if x not in cat_features]\n",
    "target_col = 'target'\n",
    "treatment_col = 'treatment_size'\n",
    "group_col = 'is_control_group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "log_interval = 20\n",
    "\n",
    "emb_cols = cat_features\n",
    "n_cont = len(cont_feats)\n",
    "emb_szs = [(None, 3), (None, 2), (None, 10)] # set according nunique values for each categorical feature\n",
    "max_size = 30\n",
    "treatments = [0, 10, 20, 30] # possible treatment sizes. (easily scalable to continuous case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutcomeDataset(data.Dataset):\n",
    "    def __init__(self, *, X, emb_cols, cont_feats):\n",
    "        \"\"\"\n",
    "\n",
    "        :param X: DataFrame with all features\n",
    "        :param emb_cols: Categorical features names\n",
    "        :param cont_feats: Continuous features names\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        self.ts = X['treatment_size'].values.astype(int)\n",
    "        self.X1 = X[emb_cols].copy().values.astype(np.int64) #categorical columns\n",
    "        self.X2 = X[cont_feats].copy().values.astype(np.float32) #numerical columns\n",
    "\n",
    "        self.y = X['target'].values.astype(int)\n",
    "        self.primary_key = X['primary_key'].values.astype(np.int64) # primary_key - object id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.ts[idx], self.X1[idx], self.X2[idx], self.y[idx], self.primary_key[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = OutcomeDataset(X=train, emb_cols=cat_feats, cont_feats=cont_feats)\n",
    "valid_ds = OutcomeDataset(X=valid, emb_cols=cat_feats, cont_feats=cont_feats)\n",
    "\n",
    "train_loader = data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = data.DataLoader(valid_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, *, \n",
    "                 output_dim: int, # First layer size\n",
    "                 output_control_dim: int, # Control group layer size\n",
    "                 output_treatment_dim: int,  # Treatment group layer size\n",
    "                 output_size_dim: int,  #Treatment_size layer size\n",
    "                 p: float, # probability for dropout\n",
    "                 embedding_sizes: list,\n",
    "                 n_cont: int,\n",
    "                 treatments: list):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.treatments = treatments\n",
    "        self.drop_layer = nn.Dropout(p=p)\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes]) # эмбединги для категориальных фичей\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings) \n",
    "        self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        \n",
    "        self.first_layer = nn.Linear(in_features=self.n_emb + self.n_cont, out_features=output_dim)\n",
    "        self.bn = nn.BatchNorm1d(self.n_cont)\n",
    "        \n",
    "        self.control_layer = nn.Linear(in_features=output_dim, out_features=output_control_dim)\n",
    "        self.treatment_layer = nn.Linear(in_features=output_dim, out_features=output_treatment_dim)\n",
    "        \n",
    "        d = dict.fromkeys([str(x) for x in self.treatments])\n",
    "        for i in d:\n",
    "            d[i]=nn.Linear(in_features=output_treatment_dim, out_features=output_size_dim)\n",
    "        self.size_layers = nn.ModuleDict(d) # layers for treatment sizes\n",
    "        \n",
    "        \n",
    "        self.output_layer = nn.Linear(in_features=output_size_dim, out_features=1)\n",
    "        self.output_control_layer = nn.Linear(in_features=output_control_dim, out_features=1)\n",
    "        \n",
    "    def forward(self, x_cat, x_cont, y, ts, primary_key):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x_cat: categorical features\n",
    "        :param: x_cont: numeric features\n",
    "        :param y: target\n",
    "        :param ts: treatment size\n",
    "        :param primary_key: object id\n",
    "        \"\"\"\n",
    "        output=[]\n",
    "        output_y=[]\n",
    "        output_ts = []\n",
    "        output_primary_key = []\n",
    "\n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        \n",
    "        x2 = self.bn(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        \n",
    "        x = self.first_layer(x)\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        if 0 in ts:\n",
    "            x1 = self.control_layer(x[ts==0])\n",
    "            x1 = F.relu(x1)\n",
    "            x1 = self.output_control_layer(x1)\n",
    "            x1 = torch.sigmoid(x1)\n",
    "            output.append(x1)\n",
    "            \n",
    "            y_tmp = y[ts==0]\n",
    "            output_y.append(y_tmp)\n",
    "            ts_tmp = ts[ts==0]\n",
    "            output_ts.append(ts_tmp)\n",
    "            primary_key_tmp = primary_key[ts==0]\n",
    "            output_primary_key.append(primary_key_tmp)\n",
    "        \n",
    "        x2 = self.treatment_layer(x[ts!=0])\n",
    "        x2 = F.relu(x2)\n",
    "        ts_n = ts[ts!=0]\n",
    "        y_n = y[ts!=0]\n",
    "        primary_key_n = primary_key[ts!=0]\n",
    "\n",
    "        for s in self.treatments:\n",
    "            if s in ts:\n",
    "                h = self.size_layers[str(s)](x2[ts_n==s])\n",
    "                h = self.drop_layer(h)\n",
    "                h = F.relu(h)\n",
    "                h = self.output_layer(h)\n",
    "                h = torch.sigmoid(h)\n",
    "                output.append(h)\n",
    "                \n",
    "                y_tmp = y_n[ts_n==s]\n",
    "                output_y.append(y_tmp)\n",
    "                ts_tmp = ts_n[ts_n==s]\n",
    "                output_ts.append(ts_tmp)\n",
    "                primary_key_tmp = primary_key_n[ts_n==s]\n",
    "                output_primary_key.append(primary_key_tmp)\n",
    "                \n",
    "        return torch.cat((output)), (torch.cat((output_y))).type(torch.FloatTensor), output_ts, output_primary_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=MyNet(output_dim=128,\n",
    "         output_control_dim=32,\n",
    "         output_treatment_dim=32,\n",
    "         output_size_dim = 16,\n",
    "         p=0.3,\n",
    "         embedding_sizes=emb_szs,\n",
    "         n_cont=n_cont,\n",
    "         treatments=treatments)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min',\n",
    "            factor=0.5, patience=10, threshold=0.001, verbose=True)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNet(\n",
       "  (drop_layer): Dropout(p=0.3, inplace=False)\n",
       "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(3, 3)\n",
       "    (1): Embedding(2, 2)\n",
       "    (2): Embedding(17, 10)\n",
       "  )\n",
       "  (first_layer): Linear(in_features=187, out_features=128, bias=True)\n",
       "  (bn): BatchNorm1d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (control_layer): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (treatment_layer): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (size_layers): ModuleDict(\n",
       "    (1000): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (3000): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (500): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5000): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (555): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (777): Linear(in_features=32, out_features=16, bias=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (output_control_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train loop\n",
    "best_score = None\n",
    "history_train = []\n",
    "history_valid = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    slt=0\n",
    "    slv=0\n",
    "    net.train() # turn on train mode\n",
    "    for batch_idx, (ts, x_cat, x_cont, target, primary_key) in enumerate(train_loader):\n",
    "        x_cat, x_cont, target = Variable(x_cat), Variable(x_cont), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        net_out, net_y, net_ts, net_primary_key = net(x_cat, x_cont, target, ts, primary_key)\n",
    "        loss = criterion(net_out, net_y.unsqueeze(1))\n",
    "        slt+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    net.eval() # turn on eval mode\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (ts_val, x_cat_val, x_cont_val, target_val, primary_key_val) in enumerate(valid_loader):\n",
    "            x_cat_val, x_cont_val, target_val = Variable(x_cat_val), Variable(x_cont_val), Variable(target_val)\n",
    "\n",
    "            net_out_val, net_y_val, net_ts_val, net_primary_key_val = net(x_cat_val, x_cont_val, target_val, ts_val, primary_key_val)\n",
    "            loss_valid = criterion(net_out_val, net_y_val.unsqueeze(1))\n",
    "            slv+=loss_valid.item()\n",
    "    scheduler.step(loss_valid)\n",
    "\n",
    "    #  save current status if current valid loss is smaller than previous\n",
    "    checkpoint = {\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': net.cpu().state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    if best_score is None:\n",
    "        best_score=loss_valid\n",
    "    if loss_valid < best_score:\n",
    "        torch.save(checkpoint, 'model_checkpoint.pt')\n",
    "\n",
    "#     visualization\n",
    "    history_train.append(slt/len(train_loader))\n",
    "    history_valid.append(slv/len(valid_loader))\n",
    "\n",
    "    if (epoch+1)%10 == 0:\n",
    "        clear_output(True)\n",
    "        print(f\"Epoch: {epoch} | Mean loss train: {slt/len(train_loader)} | | Mean loss valid: {slv/len(valid_loader)}\")\n",
    "        plt.plot(history_train, 'b')\n",
    "        plt.plot(history_valid, 'r')\n",
    "        plt.legend(['train', 'valid'])\n",
    "        plt.show();\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(X=post):\n",
    "    X = X.copy()\n",
    "    ts = X['treatment_size'].values.astype(int)\n",
    "    X1 = X[emb_cols].copy().values.astype(np.int64) #categorical columns\n",
    "    X2 = X[cont_feats].copy().values.astype(np.float32) #numerical columns\n",
    "    y = X['target'].values.astype(int)\n",
    "    primary_key = X['primary_key'].values.astype(np.int64)\n",
    "    return ts, Variable(torch.Tensor(X1).long()), Variable(torch.Tensor(X2)), Variable(torch.Tensor(y)), primary_key\n",
    "def make_ts(value=0, size=0):\n",
    "    return np.array([value]*size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stats(X=post):\n",
    "    ts, X1, X2, y, primary_key = make_data(X=X)\n",
    "    output={}\n",
    "    size = ts.shape[0]\n",
    "    for i in treatments:\n",
    "        print(i)\n",
    "        tsto = make_ts(value=i, size=size)\n",
    "        outputs, out_y, out_ts, out_primary_key = net(X1, X2, y, tsto, primary_key)\n",
    "        output[i]=[x.item() for  x in outputs]\n",
    "    return primary_key, ts, pd.DataFrame(output), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_primary_key, ts_out, output_post, y_out = make_stats(post)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res = pd.concat([pd.DataFrame({'primary_key': output_primary_key,\n",
    "                                'treatment_size': ts_out,\n",
    "                                'target': y_out}),\n",
    "                 output_post], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}